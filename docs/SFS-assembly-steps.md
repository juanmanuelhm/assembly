This is an outline of the steps for running the assembly pipeline on sequence reads generated by the [Seattle Flu Study] on [Fred Hutch rhino]

- [Globus](#globus)
  - [Set up](#set-up)
  - [File Transfer](#file-transfer)
- [Assembly Pipeline](#assembly-pipeline)
  - [Set up](#set-up-1)
    - [Flu](#flu)
    - [hCoV-19](#hcov-19)
  - [Run assembly](#run-assembly)
  - [Completed Run](#completed-run)

## Globus
[Globus] is the data management service used by the Northwest Genomics Center (NWGC) for transferring sequence data.

### Set up
1. Contact the NWGC team in [#data-transfer-nwgc] to get access to the [NWGC Globus endpoint].
   - They've had trouble adding fredhutch.org email addresses in the past so use an alternative email
2. Log on to rhino and follow [Globus Connect Personal install] instructions to install a personal endpoint in your home directory
3. Verify the new endpoint is listed in your [personal endpoints](https://app.globus.org/endpoints?scope=administered-by-me)
4. Create a symbolic link in your home directory that points to the raw read storage:
   ```
   ln -s /fh/fast/bedford_t/seattleflu/aspera-data-backup/Flu/ <symlink-name>
   ```
5. Add the path as an accessible directory to Globus Connect Personal by adding the following line to `~/.globusonline/lta/config-paths`:
    ```
    /fh/fast/bedford_t/seattleflu/aspera-data-backup/Flu/,0,1
    ```

__Note__: Steps 3 and 4 are both required to directly transfer large files into the `fast` storage instead of your personal `home` storage, which only allows 100GB storage.

### File Transfer
1. The NWGC team posts in the [#data-transfer-nwgc] Seattle Flu Study Slack channel when they have new sequence data on Globus.
2. Go to the [NWGC Globus endpoint] and find the latest batch of reads under `/seattle_flu_project/fastq_files/`
    ![Screen shot of NWGC Globus endpoint](/docs/images/select-new-batch.png)
3. In the other panel, find your personal endpoint on rhino, select the `<symlink-name>` directory that was created earlier, and press `Start` to transfer the selected batch to rhino.
    ![Screen shot of FH rhino Globus endpoint](/docs/images/select-rhino-symlink.png/)
4. You will receive an email from Globus once the data transfer is complete.

## Assembly Pipeline

### Set up
1. Log on to rhino to extract the fastq files from the transferred batch:
    ```
    cd /fh/fast/bedford_t/seattleflu/aspera-data-backup/Flu/<batch_name>/
    tar -xvf <file_name>.tar
    ```
2. Use one of the following directories to run the assembly pipeline for flu and hCoV-19 samples:
   - Flu: `/fh/fast/bedford_t/seattleflu/assembly-separate-refs/`
   - hCoV-19: `/fh/fast/bedford_t/seattleflu/assembly-ncov/`
3. Install dependencies via Conda by running:
    ```
    conda env create -f envs/seattle-flu-environment.yaml
    ```

#### Flu
- set up the config file by running the python script `scripts/setup_config_file.py`
- This will generate a JSON config file to be used for running the assembly pipeline with snakemake

#### hCoV-19
- copy the config file `config/sars-cov-2.json` to a new file `config/<batch-name>_config.json`
- edit the new config file to set the fastq directory to the new batch directory:
    ```json
    {
        "fastq_directory": "/fh/fast/bedford_t/seattleflu/aspera-data-backup/Flu/<batch_name>/*_done/",
        ...
    }
    ```

### Run assembly
1. Make sure that files from previous assembly runs are not in the current directory.
   There should _not_ be `benchmark`, `consensus_genomes`, `summary`, and `process` directories.
   If they are present, move them to a `Batch-*` subdirectory.
   This prevents files from the new batch from being mixed in with files from previous runs.
2. Create a new tmux session for the assembly run.
   This allows you to re-attach to the assembly run even if you lose your rhino connection.
    ```
    tmux new-session -t <session-name>
    ```
3. Activate the Conda environment and run the assembly pipeline:
   ```
   conda activate seattle-flu
   snakemake -R -w 120 --snakefile Snakefile-ncov --configfile config/<batch-name>_config.json --cluster-config config/cluster.json --cluster "sbatch --nodes=1 --tasks=1 --mem={cluster.memory} --cpus-per-task={cluster.cores} --tmp={cluster.disk} --time={cluster.time} -o all_output.out" -j 20 -k
   ```

### Completed Run
1. Create a new subdirectory and move all files for completed run:
   ```
   mkdir Batch-<batch-name>
   mv benchmark/ summary/ process/ consensus-genomes -t Batch-<batch-name>/
   ```
2. Create a general summary of the run within the new subdirectory:
   ```
   cd Batch-<batch-name>/
   python3 ../scripts/backfill_genome_data/parse_summary_data.py --bamstats summary/bamstats --bowtie2 summary/bowtie2 > summary/<batch-name>_sum.ndjson
   ```
3. Get a list of samples that did not map in the run:
   ```
   cd summary/not_mapped/sars-cov-2/
   ls | cut -d. -f1
   ```


[#data-transfer-nwgc]: https://seattle-flu-study.slack.com/archives/CJU8RN2Q6
[Fred Hutch rhino]: https://sciwiki.fredhutch.org/scicomputing/compute_platforms/#rhino
[Globus]: https://www.globus.org/
[Globus Connect Personal install]: https://docs.globus.org/how-to/globus-connect-personal-linux/
[NWGC Globus endpoint]: https://app.globus.org/file-manager?origin_id=178d2980-769b-11e9-8e59-029d279f7e24&origin_path=%2Fseattle_flu_project%2F
[Seattle Flu Study]: https://seattleflu.org/
